{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python373jvsc74a57bd0192b2adcf89bab941b7e96bafb904814dc6d17ff788fb22c8b7040cb8680f0f5",
   "display_name": "Python 3.7.3 64-bit ('cs224w': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "192b2adcf89bab941b7e96bafb904814dc6d17ff788fb22c8b7040cb8680f0f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import RobertaConfig, RobertaForSequenceClassification, RobertaTokenizerFast, Trainer, TrainingArguments\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from preprocess import preprocess_data, get_dataset_and_labels\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RobertaConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"roberta\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.5.1\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "config = RobertaConfig()\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = 512\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load model and tokenizer and define length of the text sequence\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base',\n",
    "                                                           gradient_checkpointing=False,\n",
    "                                                           )\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base', max_length = max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RobertaConfig {\n",
       "  \"_name_or_path\": \"roberta-base\",\n",
       "  \"architectures\": [\n",
       "    \"RobertaForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 514,\n",
       "  \"model_type\": \"roberta\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.5.1\",\n",
       "  \"type_vocab_size\": 1,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50265\n",
       "}"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_label(label):\n",
    "    if label == \"rumour\":\n",
    "        return 1\n",
    "    elif label == \"non-rumour\":\n",
    "        return 0\n",
    "    else:\n",
    "        raise Exception(\"label classes must be 'rumour' or 'non-rumour'\")\n",
    "\n",
    "\n",
    "def get_labels(label_path, sourceIds):\n",
    "    with open(label_path) as f:\n",
    "        labels = json.load(f)\n",
    "    corresponding_labels = [labels[id] for id in sourceIds]\n",
    "    numeric_labels = [convert_label(label) for label in corresponding_labels]\n",
    "\n",
    "    return numeric_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./project_data/train.data.jsonl\"\n",
    "labels_path = \"./project_data/train.label.json\"\n",
    "\n",
    "data_val_path = './project_data/dev.data.jsonl'\n",
    "val_labels_path = './project_data/dev.label.json'\n",
    "\n",
    "test_path = \"./project_data/test.data.jsonl\"\n",
    "\n",
    "texts, labels = get_dataset_and_labels(data_path=data_path, label_path=labels_path, max_sequence_length=max_sequence_length)\n",
    "val_texts, val_sourceIds = preprocess_data(data_path=data_val_path, max_sequence_length=max_sequence_length)\n",
    "val_labels = get_labels(val_labels_path, val_sourceIds)\n",
    "\n",
    "test_texts, sourceIds = preprocess_data(data_path=test_path, max_sequence_length=max_sequence_length) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=.2)\n",
    "train_texts = texts\n",
    "train_labels = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_texts, padding = 'max_length', truncation=True, max_length = max_sequence_length)\n",
    "val_encodings = tokenizer(val_texts, padding = 'max_length', truncation=True, max_length = max_sequence_length)\n",
    "test_encodings = tokenizer(test_texts, padding = 'max_length', truncation=True, max_length = max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "class TestDataset:\n",
    "    def __init__(self, tokenized_texts):\n",
    "        self.tokenized_texts = tokenized_texts\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_texts[\"input_ids\"])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.tokenized_texts.items()}\n",
    "\n",
    "train_dataset = TwitterDataset(train_encodings, train_labels)\n",
    "val_dataset = TestDataset(val_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class weights: class x / size of largest class\n",
    "\n",
    "def make_weights(train_labels):\n",
    "    unique_labels = np.unique(train_labels)\n",
    "    class_sizes_dict = {x: (np.array(train_labels) == x).sum() for x in unique_labels}\n",
    "    max_class_size = max(class_sizes_dict.values())\n",
    "    weights = torch.tensor([max_class_size / class_sizes_dict[x] for x in unique_labels]).float()\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([1.0000, 1.9318], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "class_weights = make_weights(train_labels).cuda()\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs[\"labels\"]\n",
    "        outputs = model(input_ids=inputs[\"input_ids\"])\n",
    "        logits = outputs.logits\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "        try:\n",
    "            loss = loss_fct(logits.view(-1, self.model.config.num_labels),\n",
    "                            labels.long())\n",
    "        except Exception as err:\n",
    "            print(\"\\n\")\n",
    "            print(labels)\n",
    "            print(\"\\n\")\n",
    "            print(labels.float().view(-1, self.model.config.num_labels))\n",
    "            print(\"\\n\")\n",
    "            print(logits.view(-1, self.model.config.num_labels))\n",
    "            print(err)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      ".8096439585085506e-05, 'epoch': 4.78}\n",
      " 68%|██████▊   | 2785/4067 [06:45<03:06,  6.88it/s]{'loss': 0.0158, 'learning_rate': 1.79843005326605e-05, 'epoch': 4.79}\n",
      " 69%|██████▊   | 2793/4067 [06:46<03:05,  6.87it/s]{'loss': 0.0048, 'learning_rate': 1.7872161480235495e-05, 'epoch': 4.81}\n",
      " 69%|██████▉   | 2801/4067 [06:48<03:04,  6.87it/s]{'loss': 0.3357, 'learning_rate': 1.7760022427810486e-05, 'epoch': 4.82}\n",
      " 69%|██████▉   | 2809/4067 [06:49<03:01,  6.94it/s]{'loss': 0.1345, 'learning_rate': 1.764788337538548e-05, 'epoch': 4.83}\n",
      " 69%|██████▉   | 2817/4067 [06:50<03:01,  6.88it/s]{'loss': 0.166, 'learning_rate': 1.7535744322960472e-05, 'epoch': 4.85}\n",
      " 69%|██████▉   | 2825/4067 [06:51<03:01,  6.83it/s]{'loss': 0.2941, 'learning_rate': 1.7423605270535463e-05, 'epoch': 4.86}\n",
      " 70%|██████▉   | 2833/4067 [06:52<03:02,  6.75it/s]{'loss': 0.256, 'learning_rate': 1.7311466218110458e-05, 'epoch': 4.87}\n",
      " 70%|██████▉   | 2841/4067 [06:53<02:59,  6.84it/s]{'loss': 0.2003, 'learning_rate': 1.719932716568545e-05, 'epoch': 4.89}\n",
      " 70%|███████   | 2849/4067 [06:54<02:57,  6.85it/s]{'loss': 0.1026, 'learning_rate': 1.7087188113260443e-05, 'epoch': 4.9}\n",
      " 70%|███████   | 2857/4067 [06:56<02:54,  6.93it/s]{'loss': 0.0631, 'learning_rate': 1.6975049060835438e-05, 'epoch': 4.92}\n",
      " 70%|███████   | 2865/4067 [06:57<02:55,  6.86it/s]{'loss': 0.2993, 'learning_rate': 1.686291000841043e-05, 'epoch': 4.93}\n",
      " 71%|███████   | 2873/4067 [06:58<02:53,  6.89it/s]{'loss': 0.2086, 'learning_rate': 1.6750770955985423e-05, 'epoch': 4.94}\n",
      " 71%|███████   | 2881/4067 [06:59<02:51,  6.93it/s]{'loss': 0.0045, 'learning_rate': 1.6638631903560414e-05, 'epoch': 4.96}\n",
      " 71%|███████   | 2889/4067 [07:00<02:51,  6.86it/s]{'loss': 0.1658, 'learning_rate': 1.652649285113541e-05, 'epoch': 4.97}\n",
      " 71%|███████   | 2897/4067 [07:01<02:50,  6.84it/s]{'loss': 0.2144, 'learning_rate': 1.6414353798710403e-05, 'epoch': 4.98}\n",
      " 71%|███████▏  | 2904/4067 [07:02<02:46,  6.97it/s]{'loss': 0.1367, 'learning_rate': 1.6302214746285394e-05, 'epoch': 5.0}\n",
      " 72%|███████▏  | 2913/4067 [07:04<02:46,  6.93it/s]{'loss': 0.6582, 'learning_rate': 1.619007569386039e-05, 'epoch': 5.01}\n",
      " 72%|███████▏  | 2921/4067 [07:05<02:45,  6.92it/s]{'loss': 0.0342, 'learning_rate': 1.607793664143538e-05, 'epoch': 5.03}\n",
      " 72%|███████▏  | 2929/4067 [07:06<02:44,  6.91it/s]{'loss': 0.1888, 'learning_rate': 1.596579758901037e-05, 'epoch': 5.04}\n",
      " 72%|███████▏  | 2937/4067 [07:07<02:43,  6.90it/s]{'loss': 0.0036, 'learning_rate': 1.5853658536585366e-05, 'epoch': 5.05}\n",
      " 72%|███████▏  | 2945/4067 [07:08<02:42,  6.92it/s]{'loss': 0.0032, 'learning_rate': 1.574151948416036e-05, 'epoch': 5.07}\n",
      " 73%|███████▎  | 2953/4067 [07:09<02:41,  6.91it/s]{'loss': 0.1645, 'learning_rate': 1.562938043173535e-05, 'epoch': 5.08}\n",
      " 73%|███████▎  | 2961/4067 [07:11<02:40,  6.91it/s]{'loss': 0.1277, 'learning_rate': 1.5517241379310346e-05, 'epoch': 5.09}\n",
      " 73%|███████▎  | 2969/4067 [07:12<02:38,  6.93it/s]{'loss': 0.0996, 'learning_rate': 1.5405102326885337e-05, 'epoch': 5.11}\n",
      " 73%|███████▎  | 2977/4067 [07:13<02:38,  6.89it/s]{'loss': 0.0799, 'learning_rate': 1.529296327446033e-05, 'epoch': 5.12}\n",
      " 73%|███████▎  | 2985/4067 [07:14<02:36,  6.91it/s]{'loss': 0.002, 'learning_rate': 1.5180824222035326e-05, 'epoch': 5.14}\n",
      " 74%|███████▎  | 2993/4067 [07:15<02:36,  6.87it/s]{'loss': 0.0601, 'learning_rate': 1.5068685169610317e-05, 'epoch': 5.15}\n",
      " 74%|███████▍  | 3001/4067 [07:16<02:33,  6.94it/s]{'loss': 0.0026, 'learning_rate': 1.495654611718531e-05, 'epoch': 5.16}\n",
      " 74%|███████▍  | 3009/4067 [07:17<02:33,  6.89it/s]{'loss': 0.2229, 'learning_rate': 1.4844407064760304e-05, 'epoch': 5.18}\n",
      " 74%|███████▍  | 3017/4067 [07:19<02:32,  6.91it/s]{'loss': 0.0028, 'learning_rate': 1.4732268012335295e-05, 'epoch': 5.19}\n",
      " 74%|███████▍  | 3025/4067 [07:20<02:30,  6.90it/s]{'loss': 0.0744, 'learning_rate': 1.462012895991029e-05, 'epoch': 5.2}\n",
      " 75%|███████▍  | 3033/4067 [07:21<02:29,  6.91it/s]{'loss': 0.0798, 'learning_rate': 1.4507989907485284e-05, 'epoch': 5.22}\n",
      " 75%|███████▍  | 3041/4067 [07:22<02:30,  6.83it/s]{'loss': 0.003, 'learning_rate': 1.4395850855060276e-05, 'epoch': 5.23}\n",
      " 75%|███████▍  | 3049/4067 [07:23<02:26,  6.93it/s]{'loss': 0.0754, 'learning_rate': 1.4283711802635268e-05, 'epoch': 5.25}\n",
      " 75%|███████▌  | 3057/4067 [07:24<02:26,  6.90it/s]{'loss': 0.0017, 'learning_rate': 1.417157275021026e-05, 'epoch': 5.26}\n",
      " 75%|███████▌  | 3065/4067 [07:26<02:25,  6.90it/s]{'loss': 0.1994, 'learning_rate': 1.4059433697785254e-05, 'epoch': 5.27}\n",
      " 76%|███████▌  | 3073/4067 [07:27<02:24,  6.90it/s]{'loss': 0.0597, 'learning_rate': 1.3947294645360248e-05, 'epoch': 5.29}\n",
      " 76%|███████▌  | 3081/4067 [07:28<02:22,  6.90it/s]{'loss': 0.0629, 'learning_rate': 1.383515559293524e-05, 'epoch': 5.3}\n",
      " 76%|███████▌  | 3089/4067 [07:29<02:22,  6.88it/s]{'loss': 0.1314, 'learning_rate': 1.3723016540510234e-05, 'epoch': 5.31}\n",
      " 76%|███████▌  | 3097/4067 [07:30<02:20,  6.92it/s]{'loss': 0.0437, 'learning_rate': 1.3610877488085227e-05, 'epoch': 5.33}\n",
      " 76%|███████▋  | 3105/4067 [07:31<02:18,  6.94it/s]{'loss': 0.0957, 'learning_rate': 1.3498738435660218e-05, 'epoch': 5.34}\n",
      " 77%|███████▋  | 3113/4067 [07:32<02:17,  6.92it/s]{'loss': 0.0727, 'learning_rate': 1.3386599383235212e-05, 'epoch': 5.36}\n",
      " 77%|███████▋  | 3121/4067 [07:34<02:17,  6.90it/s]{'loss': 0.0197, 'learning_rate': 1.3274460330810204e-05, 'epoch': 5.37}\n",
      " 77%|███████▋  | 3129/4067 [07:35<02:16,  6.87it/s]{'loss': 0.0015, 'learning_rate': 1.3162321278385198e-05, 'epoch': 5.38}\n",
      " 77%|███████▋  | 3137/4067 [07:36<02:14,  6.90it/s]{'loss': 0.0886, 'learning_rate': 1.3050182225960193e-05, 'epoch': 5.4}\n",
      " 77%|███████▋  | 3145/4067 [07:37<02:13,  6.92it/s]{'loss': 0.139, 'learning_rate': 1.2938043173535184e-05, 'epoch': 5.41}\n",
      " 78%|███████▊  | 3153/4067 [07:38<02:12,  6.91it/s]{'loss': 0.0012, 'learning_rate': 1.2825904121110176e-05, 'epoch': 5.43}\n",
      " 78%|███████▊  | 3161/4067 [07:39<02:11,  6.87it/s]{'loss': 0.1095, 'learning_rate': 1.2713765068685171e-05, 'epoch': 5.44}\n",
      " 78%|███████▊  | 3169/4067 [07:41<02:09,  6.92it/s]{'loss': 0.0957, 'learning_rate': 1.2601626016260162e-05, 'epoch': 5.45}\n",
      " 78%|███████▊  | 3177/4067 [07:42<02:09,  6.89it/s]{'loss': 0.2025, 'learning_rate': 1.2489486963835157e-05, 'epoch': 5.47}\n",
      " 78%|███████▊  | 3185/4067 [07:43<02:07,  6.89it/s]{'loss': 0.0781, 'learning_rate': 1.237734791141015e-05, 'epoch': 5.48}\n",
      " 79%|███████▊  | 3193/4067 [07:44<02:05,  6.94it/s]{'loss': 0.0062, 'learning_rate': 1.2265208858985142e-05, 'epoch': 5.49}\n",
      " 79%|███████▊  | 3201/4067 [07:45<02:05,  6.90it/s]{'loss': 0.0052, 'learning_rate': 1.2153069806560135e-05, 'epoch': 5.51}\n",
      " 79%|███████▉  | 3209/4067 [07:46<02:03,  6.93it/s]{'loss': 0.1681, 'learning_rate': 1.2040930754135128e-05, 'epoch': 5.52}\n",
      " 79%|███████▉  | 3217/4067 [07:47<02:03,  6.90it/s]{'loss': 0.0706, 'learning_rate': 1.192879170171012e-05, 'epoch': 5.54}\n",
      " 79%|███████▉  | 3225/4067 [07:49<02:01,  6.93it/s]{'loss': 0.2346, 'learning_rate': 1.1816652649285113e-05, 'epoch': 5.55}\n",
      " 79%|███████▉  | 3233/4067 [07:50<02:00,  6.91it/s]{'loss': 0.1219, 'learning_rate': 1.1704513596860108e-05, 'epoch': 5.56}\n",
      " 80%|███████▉  | 3241/4067 [07:51<01:57,  7.03it/s]{'loss': 0.0437, 'learning_rate': 1.15923745444351e-05, 'epoch': 5.58}\n",
      " 80%|███████▉  | 3249/4067 [07:52<01:58,  6.91it/s]{'loss': 0.4229, 'learning_rate': 1.1480235492010093e-05, 'epoch': 5.59}\n",
      " 80%|████████  | 3257/4067 [07:53<01:56,  6.94it/s]{'loss': 0.0009, 'learning_rate': 1.1368096439585086e-05, 'epoch': 5.6}\n",
      " 80%|████████  | 3265/4067 [07:54<01:55,  6.93it/s]{'loss': 0.284, 'learning_rate': 1.1255957387160079e-05, 'epoch': 5.62}\n",
      " 80%|████████  | 3273/4067 [07:55<01:55,  6.89it/s]{'loss': 0.0015, 'learning_rate': 1.1143818334735072e-05, 'epoch': 5.63}\n",
      " 81%|████████  | 3281/4067 [07:57<01:54,  6.89it/s]{'loss': 0.0629, 'learning_rate': 1.1031679282310065e-05, 'epoch': 5.65}\n",
      " 81%|████████  | 3289/4067 [07:58<01:52,  6.90it/s]{'loss': 0.0009, 'learning_rate': 1.091954022988506e-05, 'epoch': 5.66}\n",
      " 81%|████████  | 3297/4067 [07:59<01:51,  6.89it/s]{'loss': 0.0017, 'learning_rate': 1.0807401177460052e-05, 'epoch': 5.67}\n",
      " 81%|████████▏ | 3305/4067 [08:00<01:50,  6.87it/s]{'loss': 0.119, 'learning_rate': 1.0695262125035043e-05, 'epoch': 5.69}\n",
      " 81%|████████▏ | 3313/4067 [08:01<01:48,  6.92it/s]{'loss': 0.0017, 'learning_rate': 1.0583123072610036e-05, 'epoch': 5.7}\n",
      " 82%|████████▏ | 3321/4067 [08:02<01:47,  6.92it/s]{'loss': 0.0008, 'learning_rate': 1.047098402018503e-05, 'epoch': 5.71}\n",
      " 82%|████████▏ | 3329/4067 [08:03<01:46,  6.91it/s]{'loss': 0.2018, 'learning_rate': 1.0358844967760023e-05, 'epoch': 5.73}\n",
      " 82%|████████▏ | 3337/4067 [08:05<01:47,  6.81it/s]{'loss': 0.0033, 'learning_rate': 1.0246705915335016e-05, 'epoch': 5.74}\n",
      " 82%|████████▏ | 3345/4067 [08:06<01:45,  6.82it/s]{'loss': 0.0012, 'learning_rate': 1.0134566862910009e-05, 'epoch': 5.76}\n",
      " 82%|████████▏ | 3353/4067 [08:07<01:43,  6.91it/s]{'loss': 0.001, 'learning_rate': 1.0022427810485002e-05, 'epoch': 5.77}\n",
      " 83%|████████▎ | 3361/4067 [08:08<01:44,  6.78it/s]{'loss': 0.0983, 'learning_rate': 9.910288758059994e-06, 'epoch': 5.78}\n",
      " 83%|████████▎ | 3369/4067 [08:09<01:41,  6.90it/s]{'loss': 0.058, 'learning_rate': 9.798149705634987e-06, 'epoch': 5.8}\n",
      " 83%|████████▎ | 3377/4067 [08:10<01:39,  6.93it/s]{'loss': 0.2295, 'learning_rate': 9.686010653209982e-06, 'epoch': 5.81}\n",
      " 83%|████████▎ | 3385/4067 [08:12<01:38,  6.94it/s]{'loss': 0.0012, 'learning_rate': 9.573871600784974e-06, 'epoch': 5.82}\n",
      " 83%|████████▎ | 3393/4067 [08:13<01:37,  6.92it/s]{'loss': 0.2229, 'learning_rate': 9.461732548359967e-06, 'epoch': 5.84}\n",
      " 84%|████████▎ | 3401/4067 [08:14<01:36,  6.88it/s]{'loss': 0.0728, 'learning_rate': 9.34959349593496e-06, 'epoch': 5.85}\n",
      " 84%|████████▍ | 3409/4067 [08:15<01:35,  6.92it/s]{'loss': 0.0014, 'learning_rate': 9.237454443509953e-06, 'epoch': 5.87}\n",
      " 84%|████████▍ | 3417/4067 [08:16<01:34,  6.88it/s]{'loss': 0.0489, 'learning_rate': 9.125315391084946e-06, 'epoch': 5.88}\n",
      " 84%|████████▍ | 3425/4067 [08:17<01:33,  6.87it/s]{'loss': 0.093, 'learning_rate': 9.013176338659938e-06, 'epoch': 5.89}\n",
      " 84%|████████▍ | 3433/4067 [08:18<01:32,  6.87it/s]{'loss': 0.0114, 'learning_rate': 8.901037286234931e-06, 'epoch': 5.91}\n",
      " 85%|████████▍ | 3441/4067 [08:20<01:30,  6.92it/s]{'loss': 0.0009, 'learning_rate': 8.788898233809926e-06, 'epoch': 5.92}\n",
      " 85%|████████▍ | 3449/4067 [08:21<01:29,  6.91it/s]{'loss': 0.0032, 'learning_rate': 8.676759181384919e-06, 'epoch': 5.93}\n",
      " 85%|████████▌ | 3457/4067 [08:22<01:28,  6.87it/s]{'loss': 0.0274, 'learning_rate': 8.56462012895991e-06, 'epoch': 5.95}\n",
      " 85%|████████▌ | 3465/4067 [08:23<01:26,  6.92it/s]{'loss': 0.0009, 'learning_rate': 8.452481076534903e-06, 'epoch': 5.96}\n",
      " 85%|████████▌ | 3473/4067 [08:24<01:25,  6.94it/s]{'loss': 0.0006, 'learning_rate': 8.340342024109897e-06, 'epoch': 5.98}\n",
      " 86%|████████▌ | 3481/4067 [08:25<01:24,  6.93it/s]{'loss': 0.0006, 'learning_rate': 8.22820297168489e-06, 'epoch': 5.99}\n",
      " 86%|████████▌ | 3489/4067 [08:26<01:20,  7.14it/s]{'loss': 0.1622, 'learning_rate': 8.116063919259883e-06, 'epoch': 6.0}\n",
      " 86%|████████▌ | 3497/4067 [08:28<01:22,  6.90it/s]{'loss': 0.0758, 'learning_rate': 8.003924866834875e-06, 'epoch': 6.02}\n",
      " 86%|████████▌ | 3505/4067 [08:29<01:21,  6.92it/s]{'loss': 0.0007, 'learning_rate': 7.891785814409868e-06, 'epoch': 6.03}\n",
      " 86%|████████▋ | 3513/4067 [08:30<01:19,  6.96it/s]{'loss': 0.0006, 'learning_rate': 7.779646761984861e-06, 'epoch': 6.04}\n",
      " 87%|████████▋ | 3521/4067 [08:31<01:19,  6.90it/s]{'loss': 0.1301, 'learning_rate': 7.667507709559854e-06, 'epoch': 6.06}\n",
      " 87%|████████▋ | 3529/4067 [08:32<01:17,  6.95it/s]{'loss': 0.0006, 'learning_rate': 7.555368657134848e-06, 'epoch': 6.07}\n",
      " 87%|████████▋ | 3537/4067 [08:33<01:16,  6.90it/s]{'loss': 0.0005, 'learning_rate': 7.443229604709841e-06, 'epoch': 6.09}\n",
      " 87%|████████▋ | 3545/4067 [08:35<01:15,  6.91it/s]{'loss': 0.0241, 'learning_rate': 7.331090552284833e-06, 'epoch': 6.1}\n",
      " 87%|████████▋ | 3553/4067 [08:36<01:15,  6.82it/s]{'loss': 0.0012, 'learning_rate': 7.218951499859826e-06, 'epoch': 6.11}\n",
      " 88%|████████▊ | 3561/4067 [08:37<01:13,  6.84it/s]{'loss': 0.0004, 'learning_rate': 7.10681244743482e-06, 'epoch': 6.13}\n",
      " 88%|████████▊ | 3569/4067 [08:38<01:12,  6.88it/s]{'loss': 0.1623, 'learning_rate': 6.994673395009812e-06, 'epoch': 6.14}\n",
      " 88%|████████▊ | 3577/4067 [08:39<01:10,  6.94it/s]{'loss': 0.0578, 'learning_rate': 6.882534342584805e-06, 'epoch': 6.15}\n",
      " 88%|████████▊ | 3585/4067 [08:40<01:09,  6.92it/s]{'loss': 0.0005, 'learning_rate': 6.770395290159798e-06, 'epoch': 6.17}\n",
      " 88%|████████▊ | 3593/4067 [08:41<01:08,  6.93it/s]{'loss': 0.0004, 'learning_rate': 6.6582562377347916e-06, 'epoch': 6.18}\n",
      " 89%|████████▊ | 3601/4067 [08:43<01:07,  6.94it/s]{'loss': 0.1744, 'learning_rate': 6.546117185309784e-06, 'epoch': 6.2}\n",
      " 89%|████████▊ | 3609/4067 [08:44<01:06,  6.90it/s]{'loss': 0.0999, 'learning_rate': 6.433978132884777e-06, 'epoch': 6.21}\n",
      " 89%|████████▉ | 3617/4067 [08:45<01:04,  6.93it/s]{'loss': 0.0006, 'learning_rate': 6.321839080459771e-06, 'epoch': 6.22}\n",
      " 89%|████████▉ | 3625/4067 [08:46<01:04,  6.82it/s]{'loss': 0.102, 'learning_rate': 6.209700028034764e-06, 'epoch': 6.24}\n",
      " 89%|████████▉ | 3633/4067 [08:47<01:02,  6.89it/s]{'loss': 0.0768, 'learning_rate': 6.0975609756097564e-06, 'epoch': 6.25}\n",
      " 90%|████████▉ | 3641/4067 [08:48<01:01,  6.91it/s]{'loss': 0.0008, 'learning_rate': 5.985421923184749e-06, 'epoch': 6.27}\n",
      " 90%|████████▉ | 3649/4067 [08:50<01:00,  6.88it/s]{'loss': 0.0018, 'learning_rate': 5.873282870759742e-06, 'epoch': 6.28}\n",
      " 90%|████████▉ | 3657/4067 [08:51<00:59,  6.92it/s]{'loss': 0.0008, 'learning_rate': 5.761143818334736e-06, 'epoch': 6.29}\n",
      " 90%|█████████ | 3665/4067 [08:52<00:58,  6.89it/s]{'loss': 0.0055, 'learning_rate': 5.6490047659097285e-06, 'epoch': 6.31}\n",
      " 90%|█████████ | 3673/4067 [08:53<00:57,  6.89it/s]{'loss': 0.0006, 'learning_rate': 5.536865713484721e-06, 'epoch': 6.32}\n",
      " 91%|█████████ | 3681/4067 [08:54<00:56,  6.88it/s]{'loss': 0.0007, 'learning_rate': 5.424726661059714e-06, 'epoch': 6.33}\n",
      " 91%|█████████ | 3689/4067 [08:55<00:54,  6.92it/s]{'loss': 0.0661, 'learning_rate': 5.312587608634708e-06, 'epoch': 6.35}\n",
      " 91%|█████████ | 3697/4067 [08:56<00:53,  6.86it/s]{'loss': 0.001, 'learning_rate': 5.2004485562097e-06, 'epoch': 6.36}\n",
      " 91%|█████████ | 3705/4067 [08:58<00:52,  6.91it/s]{'loss': 0.1014, 'learning_rate': 5.088309503784693e-06, 'epoch': 6.38}\n",
      " 91%|█████████▏| 3713/4067 [08:59<00:51,  6.90it/s]{'loss': 0.0861, 'learning_rate': 4.976170451359686e-06, 'epoch': 6.39}\n",
      " 91%|█████████▏| 3721/4067 [09:00<00:50,  6.92it/s]{'loss': 0.0006, 'learning_rate': 4.864031398934679e-06, 'epoch': 6.4}\n",
      " 92%|█████████▏| 3729/4067 [09:01<00:49,  6.86it/s]{'loss': 0.0701, 'learning_rate': 4.751892346509673e-06, 'epoch': 6.42}\n",
      " 92%|█████████▏| 3737/4067 [09:02<00:47,  6.91it/s]{'loss': 0.0228, 'learning_rate': 4.639753294084665e-06, 'epoch': 6.43}\n",
      " 92%|█████████▏| 3745/4067 [09:03<00:46,  6.92it/s]{'loss': 0.0009, 'learning_rate': 4.527614241659658e-06, 'epoch': 6.44}\n",
      " 92%|█████████▏| 3753/4067 [09:04<00:45,  6.94it/s]{'loss': 0.0004, 'learning_rate': 4.415475189234651e-06, 'epoch': 6.46}\n",
      " 92%|█████████▏| 3761/4067 [09:06<00:44,  6.86it/s]{'loss': 0.0715, 'learning_rate': 4.303336136809645e-06, 'epoch': 6.47}\n",
      " 93%|█████████▎| 3769/4067 [09:07<00:43,  6.91it/s]{'loss': 0.0558, 'learning_rate': 4.191197084384637e-06, 'epoch': 6.49}\n",
      " 93%|█████████▎| 3777/4067 [09:08<00:42,  6.87it/s]{'loss': 0.0006, 'learning_rate': 4.07905803195963e-06, 'epoch': 6.5}\n",
      " 93%|█████████▎| 3785/4067 [09:09<00:40,  6.93it/s]{'loss': 0.0006, 'learning_rate': 3.966918979534623e-06, 'epoch': 6.51}\n",
      " 93%|█████████▎| 3793/4067 [09:10<00:39,  6.92it/s]{'loss': 0.0006, 'learning_rate': 3.854779927109616e-06, 'epoch': 6.53}\n",
      " 93%|█████████▎| 3801/4067 [09:11<00:38,  6.94it/s]{'loss': 0.0467, 'learning_rate': 3.7426408746846087e-06, 'epoch': 6.54}\n",
      " 94%|█████████▎| 3809/4067 [09:13<00:37,  6.94it/s]{'loss': 0.0007, 'learning_rate': 3.6305018222596023e-06, 'epoch': 6.55}\n",
      " 94%|█████████▍| 3817/4067 [09:14<00:36,  6.88it/s]{'loss': 0.0113, 'learning_rate': 3.5183627698345947e-06, 'epoch': 6.57}\n",
      " 94%|█████████▍| 3825/4067 [09:15<00:35,  6.88it/s]{'loss': 0.158, 'learning_rate': 3.406223717409588e-06, 'epoch': 6.58}\n",
      " 94%|█████████▍| 3833/4067 [09:16<00:33,  6.92it/s]{'loss': 0.0007, 'learning_rate': 3.294084664984581e-06, 'epoch': 6.6}\n",
      " 94%|█████████▍| 3841/4067 [09:17<00:32,  6.92it/s]{'loss': 0.0005, 'learning_rate': 3.181945612559574e-06, 'epoch': 6.61}\n",
      " 95%|█████████▍| 3849/4067 [09:18<00:31,  6.93it/s]{'loss': 0.0005, 'learning_rate': 3.0698065601345667e-06, 'epoch': 6.62}\n",
      " 95%|█████████▍| 3857/4067 [09:19<00:30,  6.94it/s]{'loss': 0.0004, 'learning_rate': 2.95766750770956e-06, 'epoch': 6.64}\n",
      " 95%|█████████▌| 3865/4067 [09:21<00:29,  6.93it/s]{'loss': 0.1605, 'learning_rate': 2.8455284552845528e-06, 'epoch': 6.65}\n",
      " 95%|█████████▌| 3873/4067 [09:22<00:28,  6.87it/s]{'loss': 0.0007, 'learning_rate': 2.733389402859546e-06, 'epoch': 6.66}\n",
      " 95%|█████████▌| 3881/4067 [09:23<00:26,  6.94it/s]{'loss': 0.0014, 'learning_rate': 2.6212503504345392e-06, 'epoch': 6.68}\n",
      " 96%|█████████▌| 3889/4067 [09:24<00:25,  6.90it/s]{'loss': 0.003, 'learning_rate': 2.509111298009532e-06, 'epoch': 6.69}\n",
      " 96%|█████████▌| 3897/4067 [09:25<00:24,  6.92it/s]{'loss': 0.0006, 'learning_rate': 2.3969722455845253e-06, 'epoch': 6.71}\n",
      " 96%|█████████▌| 3905/4067 [09:26<00:23,  6.90it/s]{'loss': 0.0005, 'learning_rate': 2.284833193159518e-06, 'epoch': 6.72}\n",
      " 96%|█████████▌| 3913/4067 [09:27<00:22,  6.92it/s]{'loss': 0.0005, 'learning_rate': 2.172694140734511e-06, 'epoch': 6.73}\n",
      " 96%|█████████▋| 3921/4067 [09:29<00:21,  6.89it/s]{'loss': 0.0007, 'learning_rate': 2.060555088309504e-06, 'epoch': 6.75}\n",
      " 97%|█████████▋| 3929/4067 [09:30<00:19,  6.90it/s]{'loss': 0.0004, 'learning_rate': 1.948416035884497e-06, 'epoch': 6.76}\n",
      " 97%|█████████▋| 3937/4067 [09:31<00:18,  6.91it/s]{'loss': 0.0004, 'learning_rate': 1.83627698345949e-06, 'epoch': 6.77}\n",
      " 97%|█████████▋| 3945/4067 [09:32<00:17,  6.94it/s]{'loss': 0.0004, 'learning_rate': 1.724137931034483e-06, 'epoch': 6.79}\n",
      " 97%|█████████▋| 3953/4067 [09:33<00:16,  6.91it/s]{'loss': 0.0004, 'learning_rate': 1.6119988786094757e-06, 'epoch': 6.8}\n",
      " 97%|█████████▋| 3961/4067 [09:34<00:15,  6.91it/s]{'loss': 0.0005, 'learning_rate': 1.4998598261844687e-06, 'epoch': 6.82}\n",
      " 98%|█████████▊| 3969/4067 [09:35<00:14,  6.88it/s]{'loss': 0.0008, 'learning_rate': 1.387720773759462e-06, 'epoch': 6.83}\n",
      " 98%|█████████▊| 3977/4067 [09:37<00:12,  6.93it/s]{'loss': 0.0092, 'learning_rate': 1.2755817213344548e-06, 'epoch': 6.84}\n",
      " 98%|█████████▊| 3985/4067 [09:38<00:11,  6.92it/s]{'loss': 0.0005, 'learning_rate': 1.1634426689094478e-06, 'epoch': 6.86}\n",
      " 98%|█████████▊| 3993/4067 [09:39<00:10,  6.95it/s]{'loss': 0.0007, 'learning_rate': 1.0513036164844408e-06, 'epoch': 6.87}\n",
      " 98%|█████████▊| 4001/4067 [09:40<00:09,  6.92it/s]{'loss': 0.0098, 'learning_rate': 9.391645640594337e-07, 'epoch': 6.88}\n",
      " 99%|█████████▊| 4009/4067 [09:41<00:08,  6.91it/s]{'loss': 0.0454, 'learning_rate': 8.270255116344267e-07, 'epoch': 6.9}\n",
      " 99%|█████████▉| 4017/4067 [09:42<00:07,  6.92it/s]{'loss': 0.0005, 'learning_rate': 7.148864592094197e-07, 'epoch': 6.91}\n",
      " 99%|█████████▉| 4025/4067 [09:44<00:06,  6.95it/s]{'loss': 0.0011, 'learning_rate': 6.027474067844127e-07, 'epoch': 6.93}\n",
      " 99%|█████████▉| 4033/4067 [09:45<00:04,  6.95it/s]{'loss': 0.0004, 'learning_rate': 4.906083543594056e-07, 'epoch': 6.94}\n",
      " 99%|█████████▉| 4041/4067 [09:46<00:03,  6.91it/s]{'loss': 0.1757, 'learning_rate': 3.7846930193439866e-07, 'epoch': 6.95}\n",
      "100%|█████████▉| 4049/4067 [09:47<00:02,  6.92it/s]{'loss': 0.0003, 'learning_rate': 2.6633024950939167e-07, 'epoch': 6.97}\n",
      "100%|█████████▉| 4057/4067 [09:48<00:01,  6.90it/s]{'loss': 0.0081, 'learning_rate': 1.5419119708438463e-07, 'epoch': 6.98}\n",
      "100%|█████████▉| 4065/4067 [09:49<00:00,  6.90it/s]{'loss': 0.0004, 'learning_rate': 4.2052144659377626e-08, 'epoch': 6.99}\n",
      "100%|██████████| 4067/4067 [09:50<00:00,  6.89it/s]{'train_runtime': 590.0191, 'train_samples_per_second': 6.893, 'epoch': 7.0}\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4067, training_loss=0.284460890784224, metrics={'train_runtime': 590.0191, 'train_samples_per_second': 6.893, 'epoch': 7.0})"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "# define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = './roberta_results',\n",
    "    num_train_epochs=7,\n",
    "    per_device_train_batch_size = 8,\n",
    "    gradient_accumulation_steps = 1,    \n",
    "    per_device_eval_batch_size= 8,\n",
    "    evaluation_strategy = \"no\",\n",
    "    disable_tqdm = False, \n",
    "    load_best_model_at_end=True,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps = 8,\n",
    "    fp16 = True,\n",
    "    logging_dir='./logs',\n",
    "    dataloader_num_workers = 0,\n",
    "    do_eval=False,\n",
    "    run_name = 'roberta-classification'\n",
    ")\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,                         # the instantiated Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer         # training dataset\n",
    "    # eval_dataset=val_dataset           # evaluation dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('./results/twitter-rumour-classification_roberta_512_batch4_grad16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LongformerForSequenceClassification.from_pretrained('./results/twitter-rumour-classification_1024_batch8_grad8//', local_files_only=True).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 73/73 [00:05<00:00, 14.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# from transformers import default_data_collator\n",
    "\n",
    "# label_ids: torch.Tensor = None\n",
    "# preds: torch.Tensor = None\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=8)\n",
    "\n",
    "#     for batch in tqdm(dataloader):\n",
    "\n",
    "#         batch['input_ids'] = batch['input_ids'].cuda()\n",
    "            \n",
    "#         predictions = model(input_ids=batch['input_ids']\n",
    "#                                    )\n",
    "        \n",
    "#         predictions = predictions[0]\n",
    "\n",
    "#         if preds is None:\n",
    "#             preds = predictions.detach().sigmoid()\n",
    "#         else:\n",
    "#             preds = torch.cat((preds, predictions.detach()), dim=0)\n",
    "\n",
    "\n",
    "#         # if label_ids is None:\n",
    "#         #     label_ids = batch[\"labels\"].detach()\n",
    "#         # else:\n",
    "#         #     label_ids = torch.cat((label_ids, batch[\"labels\"].detach()), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import precision_recall_fscore_support\n",
    "# predictions = np.argmax(preds.to(\"cpu\"), axis=1)\n",
    "# p, r, f, _ = precision_recall_fscore_support(predictions, val_labels, pos_label=1, average=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Precision:  0.8021390374331551\nRecall:  0.8379888268156425\nF1:  0.8196721311475409\n"
     ]
    }
   ],
   "source": [
    "# print(\"Precision: \", p)\n",
    "# print(\"Recall: \", r)\n",
    "# print(\"F1: \", f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datasets\n",
    "# imdbtrain, imdbtest_data = datasets.load_dataset('imdb', split =['train', 'test'], \n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'label': 1,\n",
       " 'text': 'Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High\\'s satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers\\' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I\\'m here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn\\'t!'}"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "# imdbtrain[0]"
   ]
  }
 ]
}