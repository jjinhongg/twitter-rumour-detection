{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import LongformerTokenizerFast, LongformerTokenizer, LongformerForSequenceClassification, Trainer, TrainingArguments, LongformerConfig\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from preprocess import preprocess_data, get_dataset_and_labels\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LongformerConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"attention_window\": 512,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"longformer\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"sep_token_id\": 2,\n",
       "  \"transformers_version\": \"4.5.1\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = LongformerConfig()\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = 256\n",
    "# device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load model and tokenizer and define length of the text sequence\n",
    "model = LongformerForSequenceClassification.from_pretrained('allenai/longformer-base-4096',\n",
    "                                                           gradient_checkpointing=False,\n",
    "                                                           attention_window = int(max_sequence_length/2))\n",
    "tokenizer = LongformerTokenizerFast.from_pretrained('allenai/longformer-base-4096', max_length = max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LongformerConfig {\n",
       "  \"_name_or_path\": \"allenai/longformer-base-4096\",\n",
       "  \"attention_mode\": \"longformer\",\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"attention_window\": [\n",
       "    128,\n",
       "    128,\n",
       "    128,\n",
       "    128,\n",
       "    128,\n",
       "    128,\n",
       "    128,\n",
       "    128,\n",
       "    128,\n",
       "    128,\n",
       "    128,\n",
       "    128\n",
       "  ],\n",
       "  \"bos_token_id\": 0,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"ignore_attention_mask\": false,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 4098,\n",
       "  \"model_type\": \"longformer\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"sep_token_id\": 2,\n",
       "  \"transformers_version\": \"4.5.1\",\n",
       "  \"type_vocab_size\": 1,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50265\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_label(label):\n",
    "    if label == \"rumour\":\n",
    "        return 1\n",
    "    elif label == \"non-rumour\":\n",
    "        return 0\n",
    "    else:\n",
    "        raise Exception(\"label classes must be 'rumour' or 'non-rumour'\")\n",
    "\n",
    "\n",
    "def get_labels(label_path, sourceIds):\n",
    "    with open(label_path) as f:\n",
    "        labels = json.load(f)\n",
    "    corresponding_labels = [labels[id] for id in sourceIds]\n",
    "    numeric_labels = [convert_label(label) for label in corresponding_labels]\n",
    "\n",
    "    return numeric_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./project_data/train.data.jsonl\"\n",
    "labels_path = \"./project_data/train.label.json\"\n",
    "\n",
    "data_val_path = './project_data/dev.data.jsonl'\n",
    "val_labels_path = './project_data/dev.label.json'\n",
    "\n",
    "test_path = \"./project_data/test.data.jsonl\"\n",
    "\n",
    "texts, labels = get_dataset_and_labels(data_path=data_path, label_path=labels_path, max_sequence_length=max_sequence_length)\n",
    "val_texts, val_sourceIds = preprocess_data(data_path=data_val_path, max_sequence_length=max_sequence_length)\n",
    "val_labels = get_labels(val_labels_path, val_sourceIds)\n",
    "\n",
    "test_texts, sourceIds = preprocess_data(data_path=test_path, max_sequence_length=max_sequence_length) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=.2)\n",
    "train_texts = texts\n",
    "train_labels = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_texts, padding = 'max_length', truncation=True, max_length = max_sequence_length)\n",
    "val_encodings = tokenizer(val_texts, padding = 'max_length', truncation=True, max_length = max_sequence_length)\n",
    "test_encodings = tokenizer(test_texts, padding = 'max_length', truncation=True, max_length = max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "class TestDataset:\n",
    "    def __init__(self, tokenized_texts):\n",
    "        self.tokenized_texts = tokenized_texts\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_texts[\"input_ids\"])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.tokenized_texts.items()}\n",
    "\n",
    "train_dataset = TwitterDataset(train_encodings, train_labels)\n",
    "val_dataset = TestDataset(val_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4/360 [00:09<14:34,  2.46s/it]{'loss': 0.7326, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.06}\n",
      "  2%|▏         | 8/360 [00:18<13:02,  2.22s/it]{'loss': 0.719, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.11}\n",
      "  3%|▎         | 12/360 [00:26<12:33,  2.16s/it]{'loss': 0.7164, 'learning_rate': 3e-06, 'epoch': 0.17}\n",
      "  4%|▍         | 16/360 [00:35<12:21,  2.16s/it]{'loss': 0.7107, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.22}\n",
      "  6%|▌         | 20/360 [00:43<11:54,  2.10s/it]{'loss': 0.6929, 'learning_rate': 5e-06, 'epoch': 0.28}\n",
      "  7%|▋         | 24/360 [00:52<11:41,  2.09s/it]{'loss': 0.6723, 'learning_rate': 6e-06, 'epoch': 0.33}\n",
      "  8%|▊         | 28/360 [01:00<11:31,  2.08s/it]{'loss': 0.6423, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.39}\n",
      "  9%|▉         | 32/360 [01:08<11:23,  2.08s/it]{'loss': 0.6653, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.44}\n",
      " 10%|█         | 36/360 [01:17<11:15,  2.08s/it]{'loss': 0.6232, 'learning_rate': 9e-06, 'epoch': 0.5}\n",
      " 11%|█         | 40/360 [01:25<11:07,  2.08s/it]{'loss': 0.6414, 'learning_rate': 1e-05, 'epoch': 0.55}\n",
      " 12%|█▏        | 44/360 [01:33<10:59,  2.09s/it]{'loss': 0.5848, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.61}\n",
      " 13%|█▎        | 48/360 [01:42<10:49,  2.08s/it]{'loss': 0.5683, 'learning_rate': 1.2e-05, 'epoch': 0.66}\n",
      " 14%|█▍        | 52/360 [01:50<10:42,  2.09s/it]{'loss': 0.4652, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.72}\n",
      " 16%|█▌        | 56/360 [01:58<10:34,  2.09s/it]{'loss': 0.3991, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.77}\n",
      " 17%|█▋        | 60/360 [02:07<10:25,  2.09s/it]{'loss': 0.5298, 'learning_rate': 1.5e-05, 'epoch': 0.83}\n",
      " 18%|█▊        | 64/360 [02:15<10:16,  2.08s/it]{'loss': 0.5656, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.88}\n",
      " 19%|█▉        | 68/360 [02:23<10:09,  2.09s/it]{'loss': 0.3881, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.94}\n",
      " 20%|██        | 72/360 [02:32<10:03,  2.10s/it]{'loss': 0.4542, 'learning_rate': 1.8e-05, 'epoch': 0.99}\n",
      " 21%|██        | 76/360 [02:41<10:29,  2.22s/it]{'loss': 0.4705, 'learning_rate': 1.9e-05, 'epoch': 1.06}\n",
      " 22%|██▏       | 80/360 [02:50<09:52,  2.12s/it]{'loss': 0.4991, 'learning_rate': 2e-05, 'epoch': 1.11}\n",
      " 23%|██▎       | 84/360 [02:58<09:37,  2.09s/it]{'loss': 0.4122, 'learning_rate': 2.1e-05, 'epoch': 1.17}\n",
      " 24%|██▍       | 88/360 [03:06<09:28,  2.09s/it]{'loss': 0.4498, 'learning_rate': 2.2000000000000003e-05, 'epoch': 1.22}\n",
      " 26%|██▌       | 92/360 [03:15<09:19,  2.09s/it]{'loss': 0.3726, 'learning_rate': 2.3000000000000003e-05, 'epoch': 1.28}\n",
      " 27%|██▋       | 96/360 [03:23<09:10,  2.08s/it]{'loss': 0.3088, 'learning_rate': 2.4e-05, 'epoch': 1.33}\n",
      " 28%|██▊       | 100/360 [03:31<09:01,  2.08s/it]{'loss': 0.4265, 'learning_rate': 2.5e-05, 'epoch': 1.39}\n",
      " 29%|██▉       | 104/360 [03:40<08:53,  2.08s/it]{'loss': 0.3358, 'learning_rate': 2.6000000000000002e-05, 'epoch': 1.44}\n",
      " 30%|███       | 108/360 [03:48<08:45,  2.09s/it]{'loss': 0.4062, 'learning_rate': 2.7000000000000002e-05, 'epoch': 1.5}\n",
      " 31%|███       | 112/360 [03:56<08:38,  2.09s/it]{'loss': 0.3649, 'learning_rate': 2.8000000000000003e-05, 'epoch': 1.55}\n",
      " 32%|███▏      | 116/360 [04:05<08:29,  2.09s/it]{'loss': 0.3814, 'learning_rate': 2.9e-05, 'epoch': 1.61}\n",
      " 33%|███▎      | 120/360 [04:13<08:20,  2.08s/it]{'loss': 0.3431, 'learning_rate': 3e-05, 'epoch': 1.66}\n",
      " 34%|███▍      | 124/360 [04:21<08:13,  2.09s/it]{'loss': 0.3906, 'learning_rate': 3.1e-05, 'epoch': 1.72}\n",
      " 36%|███▌      | 128/360 [04:30<08:03,  2.08s/it]{'loss': 0.3587, 'learning_rate': 3.2000000000000005e-05, 'epoch': 1.77}\n",
      " 37%|███▋      | 132/360 [04:38<07:56,  2.09s/it]{'loss': 0.3626, 'learning_rate': 3.3e-05, 'epoch': 1.83}\n",
      " 38%|███▊      | 136/360 [04:47<07:47,  2.09s/it]{'loss': 0.3073, 'learning_rate': 3.4000000000000007e-05, 'epoch': 1.88}\n",
      " 39%|███▉      | 140/360 [04:55<07:39,  2.09s/it]{'loss': 0.3611, 'learning_rate': 3.5e-05, 'epoch': 1.94}\n",
      " 40%|████      | 144/360 [05:03<07:35,  2.11s/it]{'loss': 0.3755, 'learning_rate': 3.6e-05, 'epoch': 1.99}\n",
      " 41%|████      | 148/360 [05:13<07:55,  2.24s/it]{'loss': 0.3185, 'learning_rate': 3.7e-05, 'epoch': 2.06}\n",
      " 42%|████▏     | 152/360 [05:21<07:22,  2.13s/it]{'loss': 0.27, 'learning_rate': 3.8e-05, 'epoch': 2.11}\n",
      " 43%|████▎     | 156/360 [05:30<07:07,  2.10s/it]{'loss': 0.2396, 'learning_rate': 3.9000000000000006e-05, 'epoch': 2.17}\n",
      " 44%|████▍     | 160/360 [05:38<06:57,  2.09s/it]{'loss': 0.3237, 'learning_rate': 4e-05, 'epoch': 2.22}\n",
      " 46%|████▌     | 164/360 [05:47<06:50,  2.09s/it]{'loss': 0.235, 'learning_rate': 4.1e-05, 'epoch': 2.28}\n",
      " 47%|████▋     | 168/360 [05:55<06:41,  2.09s/it]{'loss': 0.3077, 'learning_rate': 4.2e-05, 'epoch': 2.33}\n",
      " 48%|████▊     | 172/360 [06:03<06:32,  2.09s/it]{'loss': 0.2538, 'learning_rate': 4.3e-05, 'epoch': 2.39}\n",
      " 49%|████▉     | 176/360 [06:12<06:24,  2.09s/it]{'loss': 0.2382, 'learning_rate': 4.4000000000000006e-05, 'epoch': 2.44}\n",
      " 50%|█████     | 180/360 [06:20<06:16,  2.09s/it]{'loss': 0.2718, 'learning_rate': 4.5e-05, 'epoch': 2.5}\n",
      " 51%|█████     | 184/360 [06:28<06:07,  2.09s/it]{'loss': 0.2735, 'learning_rate': 4.600000000000001e-05, 'epoch': 2.55}\n",
      " 52%|█████▏    | 188/360 [06:37<05:59,  2.09s/it]{'loss': 0.2926, 'learning_rate': 4.7e-05, 'epoch': 2.61}\n",
      " 53%|█████▎    | 192/360 [06:45<05:52,  2.10s/it]{'loss': 0.3276, 'learning_rate': 4.8e-05, 'epoch': 2.66}\n",
      " 54%|█████▍    | 196/360 [06:54<05:42,  2.09s/it]{'loss': 0.2299, 'learning_rate': 4.9e-05, 'epoch': 2.72}\n",
      " 56%|█████▌    | 200/360 [07:02<05:33,  2.08s/it]{'loss': 0.2438, 'learning_rate': 5e-05, 'epoch': 2.77}\n",
      " 57%|█████▋    | 204/360 [07:10<05:25,  2.09s/it]{'loss': 0.2506, 'learning_rate': 4.875e-05, 'epoch': 2.83}\n",
      " 58%|█████▊    | 208/360 [07:19<05:17,  2.09s/it]{'loss': 0.2632, 'learning_rate': 4.75e-05, 'epoch': 2.88}\n",
      " 59%|█████▉    | 212/360 [07:27<05:09,  2.09s/it]{'loss': 0.2209, 'learning_rate': 4.6250000000000006e-05, 'epoch': 2.94}\n",
      " 60%|██████    | 216/360 [07:35<05:00,  2.09s/it]{'loss': 0.2978, 'learning_rate': 4.5e-05, 'epoch': 2.99}\n",
      " 61%|██████    | 220/360 [07:45<05:09,  2.21s/it]{'loss': 0.1972, 'learning_rate': 4.375e-05, 'epoch': 3.06}\n",
      " 62%|██████▏   | 224/360 [07:53<04:48,  2.12s/it]{'loss': 0.2156, 'learning_rate': 4.25e-05, 'epoch': 3.11}\n",
      " 63%|██████▎   | 228/360 [08:02<04:36,  2.09s/it]{'loss': 0.1742, 'learning_rate': 4.125e-05, 'epoch': 3.17}\n",
      " 64%|██████▍   | 232/360 [08:10<04:26,  2.08s/it]{'loss': 0.1794, 'learning_rate': 4e-05, 'epoch': 3.22}\n",
      " 66%|██████▌   | 236/360 [08:18<04:18,  2.09s/it]{'loss': 0.1526, 'learning_rate': 3.875e-05, 'epoch': 3.28}\n",
      " 67%|██████▋   | 240/360 [08:27<04:13,  2.11s/it]{'loss': 0.1811, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.33}\n",
      " 68%|██████▊   | 244/360 [08:35<04:04,  2.11s/it]{'loss': 0.1565, 'learning_rate': 3.625e-05, 'epoch': 3.39}\n",
      " 69%|██████▉   | 248/360 [08:44<03:54,  2.09s/it]{'loss': 0.1064, 'learning_rate': 3.5e-05, 'epoch': 3.44}\n",
      " 70%|███████   | 252/360 [08:52<03:45,  2.09s/it]{'loss': 0.1556, 'learning_rate': 3.375000000000001e-05, 'epoch': 3.5}\n",
      " 71%|███████   | 256/360 [09:00<03:36,  2.08s/it]{'loss': 0.2003, 'learning_rate': 3.2500000000000004e-05, 'epoch': 3.55}\n",
      " 72%|███████▏  | 260/360 [09:09<03:30,  2.10s/it]{'loss': 0.1713, 'learning_rate': 3.125e-05, 'epoch': 3.61}\n",
      " 73%|███████▎  | 264/360 [09:17<03:20,  2.09s/it]{'loss': 0.1264, 'learning_rate': 3e-05, 'epoch': 3.66}\n",
      " 74%|███████▍  | 268/360 [09:25<03:11,  2.09s/it]{'loss': 0.1473, 'learning_rate': 2.8749999999999997e-05, 'epoch': 3.72}\n",
      " 76%|███████▌  | 272/360 [09:34<03:03,  2.09s/it]{'loss': 0.1582, 'learning_rate': 2.7500000000000004e-05, 'epoch': 3.77}\n",
      " 77%|███████▋  | 276/360 [09:42<02:54,  2.08s/it]{'loss': 0.1869, 'learning_rate': 2.625e-05, 'epoch': 3.83}\n",
      " 78%|███████▊  | 280/360 [09:50<02:46,  2.09s/it]{'loss': 0.1172, 'learning_rate': 2.5e-05, 'epoch': 3.88}\n",
      " 79%|███████▉  | 284/360 [09:59<02:38,  2.09s/it]{'loss': 0.1171, 'learning_rate': 2.375e-05, 'epoch': 3.94}\n",
      " 80%|████████  | 288/360 [10:07<02:29,  2.08s/it]{'loss': 0.1193, 'learning_rate': 2.25e-05, 'epoch': 3.99}\n",
      " 81%|████████  | 292/360 [10:17<02:30,  2.21s/it]{'loss': 0.0921, 'learning_rate': 2.125e-05, 'epoch': 4.06}\n",
      " 82%|████████▏ | 296/360 [10:25<02:15,  2.12s/it]{'loss': 0.0443, 'learning_rate': 2e-05, 'epoch': 4.11}\n",
      " 83%|████████▎ | 300/360 [10:33<02:06,  2.10s/it]{'loss': 0.1249, 'learning_rate': 1.8750000000000002e-05, 'epoch': 4.17}\n",
      " 84%|████████▍ | 304/360 [10:42<01:58,  2.12s/it]{'loss': 0.0718, 'learning_rate': 1.75e-05, 'epoch': 4.22}\n",
      " 86%|████████▌ | 308/360 [10:50<01:49,  2.10s/it]{'loss': 0.0327, 'learning_rate': 1.6250000000000002e-05, 'epoch': 4.28}\n",
      " 87%|████████▋ | 312/360 [10:59<01:40,  2.10s/it]{'loss': 0.0301, 'learning_rate': 1.5e-05, 'epoch': 4.33}\n",
      " 88%|████████▊ | 316/360 [11:07<01:32,  2.10s/it]{'loss': 0.0428, 'learning_rate': 1.3750000000000002e-05, 'epoch': 4.39}\n",
      " 89%|████████▉ | 320/360 [11:15<01:24,  2.10s/it]{'loss': 0.0754, 'learning_rate': 1.25e-05, 'epoch': 4.44}\n",
      " 90%|█████████ | 324/360 [11:24<01:15,  2.10s/it]{'loss': 0.1198, 'learning_rate': 1.125e-05, 'epoch': 4.5}\n",
      " 91%|█████████ | 328/360 [11:32<01:07,  2.09s/it]{'loss': 0.0582, 'learning_rate': 1e-05, 'epoch': 4.55}\n",
      " 92%|█████████▏| 332/360 [11:41<00:58,  2.10s/it]{'loss': 0.0697, 'learning_rate': 8.75e-06, 'epoch': 4.61}\n",
      " 93%|█████████▎| 336/360 [11:49<00:50,  2.11s/it]{'loss': 0.0757, 'learning_rate': 7.5e-06, 'epoch': 4.66}\n",
      " 94%|█████████▍| 340/360 [11:57<00:41,  2.10s/it]{'loss': 0.0417, 'learning_rate': 6.25e-06, 'epoch': 4.72}\n",
      " 96%|█████████▌| 344/360 [12:06<00:33,  2.10s/it]{'loss': 0.1261, 'learning_rate': 5e-06, 'epoch': 4.77}\n",
      " 97%|█████████▋| 348/360 [12:14<00:25,  2.10s/it]{'loss': 0.0512, 'learning_rate': 3.75e-06, 'epoch': 4.83}\n",
      " 98%|█████████▊| 352/360 [12:23<00:16,  2.09s/it]{'loss': 0.0491, 'learning_rate': 2.5e-06, 'epoch': 4.88}\n",
      " 99%|█████████▉| 356/360 [12:31<00:08,  2.09s/it]{'loss': 0.0296, 'learning_rate': 1.25e-06, 'epoch': 4.94}\n",
      "100%|██████████| 360/360 [12:39<00:00,  2.11s/it]{'loss': 0.0538, 'learning_rate': 0.0, 'epoch': 4.99}\n",
      "{'train_runtime': 759.9207, 'train_samples_per_second': 0.474, 'epoch': 4.99}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=360, training_loss=0.2956389978114102, metrics={'train_runtime': 759.9207, 'train_samples_per_second': 0.474, 'epoch': 4.99})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = './results',\n",
    "    num_train_epochs = 5,\n",
    "    per_device_train_batch_size = 8,\n",
    "    gradient_accumulation_steps = 8,    \n",
    "    per_device_eval_batch_size= 16,\n",
    "    evaluation_strategy = \"no\",\n",
    "    do_eval= False,\n",
    "    disable_tqdm = False, \n",
    "    load_best_model_at_end=True,\n",
    "    warmup_steps=200,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps = 4,\n",
    "    fp16 = True,\n",
    "    logging_dir='./logs',\n",
    "    dataloader_num_workers = 0,\n",
    "    run_name = 'longformer-twitter-classification-rumour'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    # eval_dataset=val_dataset           # evaluation dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('./results/twitter-rumour-classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LongformerForSequenceClassification.from_pretrained('./results/twitter-rumour-classification/', local_files_only=True).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73/73 [00:10<00:00,  7.07it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import default_data_collator\n",
    "\n",
    "label_ids: torch.Tensor = None\n",
    "preds: torch.Tensor = None\n",
    "\n",
    "with torch.no_grad():\n",
    "    dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=8)\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "\n",
    "        batch['input_ids'] = batch['input_ids'].cuda()\n",
    "            \n",
    "        predictions = model(input_ids=batch['input_ids']\n",
    "                                   )\n",
    "        \n",
    "        predictions = predictions[0]\n",
    "\n",
    "        if preds is None:\n",
    "            preds = predictions.detach().sigmoid()\n",
    "        else:\n",
    "            preds = torch.cat((preds, predictions.detach()), dim=0)\n",
    "\n",
    "\n",
    "        # if label_ids is None:\n",
    "        #     label_ids = batch[\"labels\"].detach()\n",
    "        # else:\n",
    "        #     label_ids = torch.cat((label_ids, batch[\"labels\"].detach()), dim=0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7422,  0.2826],\n",
       "        [ 0.9566,  0.0341],\n",
       "        [ 0.9702,  0.0269],\n",
       "        ...,\n",
       "        [ 1.6122, -1.9080],\n",
       "        [-0.0710, -0.2444],\n",
       "        [ 2.9138, -3.1941]], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "predictions = np.argmax(preds.to(\"cpu\"), axis=1)\n",
    "p, r, f, _ = precision_recall_fscore_support(predictions, val_labels, pos_label=1, average=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.7967914438502673\n",
      "Recall:  0.8713450292397661\n",
      "F1:  0.8324022346368715\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision: \", p)\n",
    "print(\"Recall: \", r)\n",
    "print(\"F1: \", f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,\n",
       "        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,\n",
       "        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,\n",
       "        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "imdbtrain, imdbtest_data = datasets.load_dataset('imdb', split =['train', 'test'], \n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 1,\n",
       " 'text': 'Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High\\'s satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers\\' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I\\'m here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn\\'t!'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdbtrain[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('cs224w': conda)",
   "name": "python373jvsc74a57bd0192b2adcf89bab941b7e96bafb904814dc6d17ff788fb22c8b7040cb8680f0f5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "metadata": {
   "interpreter": {
    "hash": "192b2adcf89bab941b7e96bafb904814dc6d17ff788fb22c8b7040cb8680f0f5"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
